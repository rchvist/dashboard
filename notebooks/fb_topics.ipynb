{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Analyzing-Topical-Model-of-Facebook's-Topics-aiming-towards-Users.\" data-toc-modified-id=\"Analyzing-Topical-Model-of-Facebook's-Topics-aiming-towards-Users.-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Analyzing Topical Model of Facebook's Topics aiming towards Users.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Insert-the-path-to-the-output-of-dowload.py-below-(instead-of-/home/ubuntu..user_a.csv)\" data-toc-modified-id=\"Insert-the-path-to-the-output-of-dowload.py-below-(instead-of-/home/ubuntu..user_a.csv)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Insert the path to the output of dowload.py below (instead of /home/ubuntu..user_a.csv)</a></span></li><li><span><a href=\"#Merge-the-original-dataset-with-the-Topical-Distribution-spread\" data-toc-modified-id=\"Merge-the-original-dataset-with-the-Topical-Distribution-spread-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merge the original dataset with the Topical Distribution spread</a></span></li></ul></li><li><span><a href=\"#Topical-Distribution-Spread-from-Organic-Posts\" data-toc-modified-id=\"Topical-Distribution-Spread-from-Organic-Posts-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Topical Distribution Spread from Organic Posts</a></span></li><li><span><a href=\"#Sponsored-Posts-Topical-Spread\" data-toc-modified-id=\"Sponsored-Posts-Topical-Spread-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sponsored Posts Topical Spread</a></span></li><li><span><a href=\"#How-much-does-each-User's-gets-exposed-to-these-Political-Advertisements?\" data-toc-modified-id=\"How-much-does-each-User's-gets-exposed-to-these-Political-Advertisements?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>How much does each User's gets exposed to these Political Advertisements?</a></span></li><li><span><a href=\"#How-does-Facebook-impact-A-user's-timeline-impression-during-Political-Elections?\" data-toc-modified-id=\"How-does-Facebook-impact-A-user's-timeline-impression-during-Political-Elections?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>How does Facebook impact A user's timeline impression during Political Elections?</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-$USER--Personal-timeline-impression-exposure-(below)\" data-toc-modified-id=\"For-$USER--Personal-timeline-impression-exposure-(below)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>For $USER  Personal timeline impression exposure (below)</a></span></li><li><span><a href=\"#Sponsored-Posts-for-$User:\" data-toc-modified-id=\"Sponsored-Posts-for-$User:-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Sponsored Posts for $User:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Topical Model of Facebook's Topics aiming towards Users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# for word vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import nltk\n",
    "import pprint\n",
    "import re\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from ast import literal_eval  # to import the column texts as list\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(s):\n",
    "    try:\n",
    "        return detect(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# split into words\n",
    "\n",
    "\n",
    "def get_words(raw, language):\n",
    "    \"\"\"\n",
    "    :raw: text that has not been cleaned yet\n",
    "    output: Stemmed Tokens\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    tokens = word_tokenize(raw)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in set(stopwords.words(language))]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def get_topic_distribution(lda_model, raw_input, dictionary, language):\n",
    "    \"\"\"Return a vecor of topical distribution of each document or text. \n",
    "    :param lda_model: the output of the function gensim.models.ldamodel.LdaModel\n",
    "    :param raw_imput: raw chinese policy text or doc\n",
    "    :param dictionary: the output of corpora.Dictionary() function which is the vocab.\n",
    "    \"\"\"\n",
    "    from pandas import DataFrame\n",
    "    other_texts = [  # needs tokenized\n",
    "        get_words(raw_input, language)\n",
    "    ]\n",
    "    #dictionary = Dictionary(sentences)\n",
    "    other_corpus = [dictionary.doc2bow(text) for text in other_texts]\n",
    "    unseen_doc = other_corpus[0]\n",
    "    vector = lda_model[unseen_doc][0]\n",
    "    return(DataFrame.from_records(vector)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the path to the output of dowload.py below (instead of /home/ubuntu..user_a.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df = pd.read_csv('../sample_data/users/user_a.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users\n",
    "dict(df.user.groupby(df.user).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'rosemary-ceviche-meringue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a random sample and see language distribution\n",
    "random.seed(10)\n",
    "len_sample = 1000\n",
    "sample = df.sample(len_sample)[['texts', 'nature', 'publicationTime']]\n",
    "sample.texts = sample.texts.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['concatLanguage'] = sample.texts.apply(''.join).apply(lang_detect)\n",
    "set(sample.concatLanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.concatLanguage.groupby([sample.concatLanguage]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.concatLanguage.groupby([sample.concatLanguage]).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset posts with the same language and tokenize words\n",
    "sample = sample[(sample.concatLanguage == 'es')]\n",
    "language = 'spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to install ssl certificates for the python version you are using, checkout the web for how to do it.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate texts break into tokens for each sentence\n",
    "import time\n",
    "sample['concatenatedText'] = sample.texts.apply(''.join)\n",
    "start = time.time()\n",
    "sent = sample.concatenatedText\n",
    "\n",
    "sentences = []\n",
    "for i in sent:\n",
    "    sentences.append(get_words(i, language))\n",
    "end = time.time()\n",
    "print(end-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics\n",
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import gensim\n",
    "import os\n",
    "import dill\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import time\n",
    "\n",
    "# Create Dictionary\n",
    "start = time.time()\n",
    "id2word = corpora.Dictionary(sentences)\n",
    "dictionary = Dictionary(sentences)\n",
    "# Create Corpus\n",
    "texts = sentences\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True,\n",
    "                                            minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)\n",
    "# pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the original dataset with the Topical Distribution spread "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import time\n",
    "start = time.time()\n",
    "buckets = []\n",
    "for i in sample.concatenatedText:\n",
    "    buckets.append(get_topic_distribution(lda_model, i, dictionary, language))\n",
    "groups = []\n",
    "for i in range(0, len(buckets)):\n",
    "    groups.append(pd.Series.to_frame((buckets[i])).T)\n",
    "\n",
    "groups = pd.concat(groups).reset_index()\n",
    "DATA = pd.concat([sample.reset_index(drop=True), groups],\n",
    "                 axis=1).reset_index(drop=True)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "dill.dump_session('FBenv.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.load_session('FBenv.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.columns = [           'texts',           'nature',  'publicationTime',\n",
    "         'concatLanguage', 'concatenatedText',            'index',\n",
    "                        'Other','Politics','Advertisement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're interested only in politics, so we sum the other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIC = DATA[DATA.nature == 'organic']\n",
    "SPONSOR = DATA[DATA.nature == 'sponsored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import os\n",
    "import warnings\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "from bubbly.bubbly import add_slider_steps\n",
    "import math\n",
    "import pandas as pd\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "py.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "ORGANIC['publicationTime'] = pd.to_datetime(ORGANIC['publicationTime']).dt.date\n",
    "EX = pd.DataFrame(ORGANIC.groupby('publicationTime')[\n",
    "                  ['Politics', 'Advertisement', 'Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x = EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x=x,\n",
    "    y=EX['Advertisement'],\n",
    "    name='Advertisement',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x=x,\n",
    "    y=EX['Politics'],\n",
    "    name='Politics',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x=x,\n",
    "    y=EX['Other'],\n",
    "    name='Other',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "\n",
    "    legend=dict(x=0, y=-.13,\n",
    "                orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topical Distribution Spread from Organic Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX = pd.DataFrame(SPONSOR.groupby('publicationTime')[\n",
    "                  ['Politics', 'Advertisement', 'Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x = EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x=x,\n",
    "    y=EX['Advertisement'],\n",
    "    name='Sales Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x=x,\n",
    "    y=EX['Politics'],\n",
    "    name='Political Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x=x,\n",
    "    y=EX['Other'],\n",
    "    name='Other Ad',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "\n",
    "    legend=dict(x=0, y=-.13,\n",
    "                orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sponsored Posts Topical Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much does each User's gets exposed to these Political Advertisements? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now ananlyze the User's exposure to these Politcal, Advertisements\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = df[df['user'] == user]\n",
    "USER['publicationTime'] = pd.to_datetime(USER['publicationTime']).dt.date\n",
    "USER = USER.sort_values(by='publicationTime')\n",
    "USER['concatenatedText'] = USER.texts.apply(''.join)\n",
    "USER = USER[['concatenatedText', 'nature', 'publicationTime']]\n",
    "sent = USER.concatenatedText\n",
    "sentences = []\n",
    "for i in sent:\n",
    "    sentences.append(get_words(i, language))\n",
    "\n",
    "id2word = corpora.Dictionary(sentences)\n",
    "dictionary = Dictionary(sentences)\n",
    "# Create Corpus\n",
    "texts = sentences\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True,\n",
    "                                            minimum_probability=0.0)\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = []\n",
    "for i in USER.concatenatedText:\n",
    "    buckets.append(get_topic_distribution(lda_model, i, dictionary, language))\n",
    "groups = []\n",
    "for i in range(0, len(buckets)):\n",
    "    groups.append(pd.Series.to_frame((buckets[i])).T)\n",
    "\n",
    "groups = pd.concat(groups).reset_index()\n",
    "USER_DATA = pd.concat([USER.reset_index(drop=True), groups],\n",
    "                      axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DATA.columns = ['concatenatedText', 'nature',\n",
    "                     'publicationTime', 'index', 'Advertisement', 'Other', 'Politics']\n",
    "\n",
    "USER_ORGANIC = USER_DATA[USER_DATA.nature == 'organic']\n",
    "USER_SPONSOR = USER_DATA[USER_DATA.nature == 'sponsored']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does Facebook impact A user's timeline impression during Political Elections?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For $USER  Personal timeline impression exposure (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import chart_studio.plotly as py\n",
    "import os\n",
    "import warnings\n",
    "from plotly import tools\n",
    "from bubbly.bubbly import add_slider_steps\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "EX = pd.DataFrame(USER_ORGANIC.groupby('publicationTime')[\n",
    "                  ['Politics', 'Advertisement', 'Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x = EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x=x,\n",
    "    y=EX['Advertisement'],\n",
    "    name='Organic Forwarded Ads',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x=x,\n",
    "    y=EX['Politics'],\n",
    "    name='Organic Political Posts',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x=x,\n",
    "    y=EX['Other'],\n",
    "    name='Other Organic Posts',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "data_organic = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "\n",
    "    legend=dict(x=0, y=-.13,\n",
    "                orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data_organic, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sponsored Posts for \\$User:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX = pd.DataFrame(USER_SPONSOR.groupby('publicationTime')[\n",
    "                  ['Politics', 'Advertisement', 'Other']].mean()).reset_index(inplace=False)\n",
    "\n",
    "x = EX.publicationTime\n",
    "\n",
    "trace1 = dict(\n",
    "    x=x,\n",
    "    y=EX['Advertisement'],\n",
    "    name='Sponsored Advertisement',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(131, 90, 241)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace2 = dict(\n",
    "    x=x,\n",
    "    y=EX['Politics'],\n",
    "    name='Sponsored Politics',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(111, 231, 219)'),\n",
    "    stackgroup='one'\n",
    ")\n",
    "trace3 = dict(\n",
    "    x=x,\n",
    "    y=EX['Other'],\n",
    "    name='Other Sponsors',\n",
    "    hoverinfo='x+y',\n",
    "    mode='lines',\n",
    "    line=dict(width=0.5),\n",
    "    # color='rgb(184, 247, 212)'),\n",
    "    stackgroup='one')\n",
    "\n",
    "data_sponsored = [trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(autorange=True,\n",
    "               fixedrange=False),\n",
    "    legend=dict(x=0, y=-.13,\n",
    "                orientation=\"h\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=data_sponsored, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
